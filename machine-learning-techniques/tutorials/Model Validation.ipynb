{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Model validation is group of techniques for evaluating machine learning models on a limited data sample. It has different goals:\n",
    "\n",
    "* To estimate how well the model will generalize to an independent data sample.\n",
    "* To provide a more realistic performance estimate than training accuracy.\n",
    "* To choose between competing models.\n",
    "* To choose optimal model hyperparameters.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What process will we adopt?\n",
    "We will compare the different model validation techniques by following the same process:\n",
    "* generate a dataset with specific characteristics\n",
    "* select the best features\n",
    "* create appropriate train, validation and test sets.\n",
    "* use a simple model, namely Logistic Regression."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generated dataset\n",
    "SciKit learn offers a functions to generate datasets for testing machine learning algorithms. We will use the dataset generated by `make_classification` to test our models. Some parameters control the number of samples, the number of features, the number of classes and the number of informative features.\n",
    "\n",
    "The weights parameter allows to control the **class imbalance**. Class imbalance is a common problem in machine learning. It occurs when the number of samples in one class is **much higher** than the number of samples in the other classes.\n",
    "\n",
    "> Suprisingly, many phenomena in the real world are characterized by class imbalance. For example, in fraud detection, the number of fraudulent transactions is much lower than the number of normal transactions. In medical diagnosis, the number of healthy patients is much higher than the number of sick patients. In this case, the model will get biased towards the majority class. A so-called **majority class classifier** can achieve a high accuracy by simply predicting the majority class all the time. Due to the class imbalance, this is a very high accuracy, but the model is **useless**. Other evaluation measures like precision, recall and f-measure correctly assess the quality of the model. So, it is important to detect class imbalance and to use appropriate techniques to deal with it.\n",
    "\n",
    "In our case, we will generate a dataset with 80% of samples in the first class and 20% of samples in the second class. The class_sep parameter controls the ratio of the standard deviation of the clusters to the standard deviation of the data points within the clusters. The higher the value, the more the clusters are separated. The **more difficult** the classification problem will be.\n",
    "\n",
    "If you like to learn more about these functions, you can read the [documentation](https://scikit-learn.org/stable/datasets/sample_generators.html#generated-datasets)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "   Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  \\\n0  -1.497498  -0.299608  -1.219989   1.681631  -0.990179  -0.432622   \n1   1.534934  -0.166541   0.300961   0.444717   0.905650  -1.009402   \n2  -0.275462   0.170693  -1.173447  -2.549926  -0.310781  -0.047132   \n3   1.373582  -1.200772  -1.559655   1.768709   0.392348   0.165043   \n4   1.503549   0.719534   2.485566  -1.963150   0.213347   0.091418   \n\n   Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_11  Feature_12  \\\n0  -0.227664  -0.689011  -0.124279  -1.111270  ...   -0.169534   -0.275310   \n1   1.158062   1.746321   0.633632  -0.405843  ...    2.357880   -0.374774   \n2  -1.243003  -0.248716   0.857717   1.577051  ...    0.235457    0.859407   \n3  -0.284057   0.007847   0.952752   1.133455  ...   -0.204472    1.055510   \n4  -1.095687   1.167156   1.294975   1.459599  ...    1.462663   -1.182586   \n\n   Feature_13  Feature_14  Feature_15  Feature_16  Feature_17  Feature_18  \\\n0    2.190289    0.391065    0.359307   -0.955901   -0.704811    0.775147   \n1   -2.454937   -0.174250    3.112953   -0.460844   -0.285499   -0.831639   \n2    2.905283    0.320668    1.073274   -1.916580   -0.066942    1.170951   \n3   -2.548346    0.265981    0.712069   -2.533506    1.112621   -0.045173   \n4   -0.125499   -1.179403   -1.497323   -0.097872    1.113954   -0.889784   \n\n   Feature_19  target  \n0   -0.397073       1  \n1    0.260672       0  \n2    1.671492       0  \n3   -0.011757       0  \n4    1.844313       1  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature_0</th>\n      <th>Feature_1</th>\n      <th>Feature_2</th>\n      <th>Feature_3</th>\n      <th>Feature_4</th>\n      <th>Feature_5</th>\n      <th>Feature_6</th>\n      <th>Feature_7</th>\n      <th>Feature_8</th>\n      <th>Feature_9</th>\n      <th>...</th>\n      <th>Feature_11</th>\n      <th>Feature_12</th>\n      <th>Feature_13</th>\n      <th>Feature_14</th>\n      <th>Feature_15</th>\n      <th>Feature_16</th>\n      <th>Feature_17</th>\n      <th>Feature_18</th>\n      <th>Feature_19</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.497498</td>\n      <td>-0.299608</td>\n      <td>-1.219989</td>\n      <td>1.681631</td>\n      <td>-0.990179</td>\n      <td>-0.432622</td>\n      <td>-0.227664</td>\n      <td>-0.689011</td>\n      <td>-0.124279</td>\n      <td>-1.111270</td>\n      <td>...</td>\n      <td>-0.169534</td>\n      <td>-0.275310</td>\n      <td>2.190289</td>\n      <td>0.391065</td>\n      <td>0.359307</td>\n      <td>-0.955901</td>\n      <td>-0.704811</td>\n      <td>0.775147</td>\n      <td>-0.397073</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.534934</td>\n      <td>-0.166541</td>\n      <td>0.300961</td>\n      <td>0.444717</td>\n      <td>0.905650</td>\n      <td>-1.009402</td>\n      <td>1.158062</td>\n      <td>1.746321</td>\n      <td>0.633632</td>\n      <td>-0.405843</td>\n      <td>...</td>\n      <td>2.357880</td>\n      <td>-0.374774</td>\n      <td>-2.454937</td>\n      <td>-0.174250</td>\n      <td>3.112953</td>\n      <td>-0.460844</td>\n      <td>-0.285499</td>\n      <td>-0.831639</td>\n      <td>0.260672</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.275462</td>\n      <td>0.170693</td>\n      <td>-1.173447</td>\n      <td>-2.549926</td>\n      <td>-0.310781</td>\n      <td>-0.047132</td>\n      <td>-1.243003</td>\n      <td>-0.248716</td>\n      <td>0.857717</td>\n      <td>1.577051</td>\n      <td>...</td>\n      <td>0.235457</td>\n      <td>0.859407</td>\n      <td>2.905283</td>\n      <td>0.320668</td>\n      <td>1.073274</td>\n      <td>-1.916580</td>\n      <td>-0.066942</td>\n      <td>1.170951</td>\n      <td>1.671492</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.373582</td>\n      <td>-1.200772</td>\n      <td>-1.559655</td>\n      <td>1.768709</td>\n      <td>0.392348</td>\n      <td>0.165043</td>\n      <td>-0.284057</td>\n      <td>0.007847</td>\n      <td>0.952752</td>\n      <td>1.133455</td>\n      <td>...</td>\n      <td>-0.204472</td>\n      <td>1.055510</td>\n      <td>-2.548346</td>\n      <td>0.265981</td>\n      <td>0.712069</td>\n      <td>-2.533506</td>\n      <td>1.112621</td>\n      <td>-0.045173</td>\n      <td>-0.011757</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.503549</td>\n      <td>0.719534</td>\n      <td>2.485566</td>\n      <td>-1.963150</td>\n      <td>0.213347</td>\n      <td>0.091418</td>\n      <td>-1.095687</td>\n      <td>1.167156</td>\n      <td>1.294975</td>\n      <td>1.459599</td>\n      <td>...</td>\n      <td>1.462663</td>\n      <td>-1.182586</td>\n      <td>-0.125499</td>\n      <td>-1.179403</td>\n      <td>-1.497323</td>\n      <td>-0.097872</td>\n      <td>1.113954</td>\n      <td>-0.889784</td>\n      <td>1.844313</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 21 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a random binary classification problem\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=2, n_repeated=0,\n",
    "                           n_classes=2, n_clusters_per_class=2, weights=[0.8, 0.2], flip_y=0.01, class_sep=1.0,\n",
    "                           hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "data = pd.DataFrame(X, columns=[f'Feature_{i}' for i in range(X.shape[1])])\n",
    "data['target'] = y\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Class distribution\n",
    "\n",
    "Let's have a look at the class distribution of the generated dataset. We can see that the class distribution is indeed around 80% for the first class and 20% for the second class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0    79.8\n1    20.2\ndtype: float64"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the class distribution\n",
    "pd.Series(y).value_counts(normalize=True) * 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data set size\n",
    "As a rule of thumb the size of dataset should be an **order of magnitude larger** than the number of features. In our case, we have 1000 samples ($10^3$) and 20 features ($2 \\cdot 10^1$), so we have enough data to train a model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bias - Variance Tradeoff\n",
    "\n",
    "Overfitting and underfitting are two common problems in machine learning. They are related to the bias-variance tradeoff. A model with high bias pays little attention to the training data and oversimplifies the model. A model with high variance pays too much attention to the training data and overfits it. The goal is to find a sweet spot in the middle with a model that has low bias and low variance.\n",
    "\n",
    "A model that **overfits** data is a model that models the training data too well. It is able to predict the training data very well, but it is not able to generalize to new data. This is a problem because the model will not perform well on new data. The model is too complex and has learned the noise in the training data. The model will have high variance.\n",
    "\n",
    "A model that **underfits** the data is a model that does not capture the underlying structure of the data. It is not able to predict the training data well. This is also a problem because the model will not perform well on new data. The model is too simple and has not learned the relevant structure in the training data. This is called underfitting. The model will have high bias.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple Bias-Variance Tradeoff example\n",
    "\n",
    "See slides for more details."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiasÂ²: 1.28 +  Variance: 2.74 = MSE: 4.02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "scale_weights = np.array([65.4, 61.5, 62.5])\n",
    "bias = scale_weights.mean() - 62\n",
    "variance = ((scale_weights - scale_weights.mean()) ** 2).sum() / scale_weights.size\n",
    "mse = ((scale_weights - 62) ** 2).mean()\n",
    "print(f'BiasÂ²: {bias ** 2:.2f} +  Variance: {variance:.2f} = MSE: {mse:.2f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finding the sweet spot\n",
    "\n",
    "Finding the sweet spot is a matter of **tradeoff**. A model with low bias and low variance is a model that is able to generalize well to new data. This is the ideal scenario. A model with low bias and high variance is a model that is able to predict the training data well, but it is not able to generalize to new data. A model with high bias and low variance is a model that is not able to predict the training data well, but it is able to generalize to new data, but still underperformss.  A model with high bias and high variance is a model that is not able to predict the training data well, and it is even not able to generalize to new data.\n",
    "\n",
    "A model with high bias is said to be **underfitting**. A model with high variance is said to be **overfitting**. The goal is to find a model that has both reasonable low bias and low variance. This is the sweet spot.\n",
    "\n",
    "### Data Splitting Techniques\n",
    "To counteract **overfitting**, we can use data splitting techniques. We split the original data into a:\n",
    "* **Training set**: The data used to fit the model. Overfitting occurs when the model fits the training data too well.\n",
    "* **Validation set**: The data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. If the model performs well on the validation set, it is likely to perform well on new data and no overfitting has occurred.\n",
    "* **Test set**: The data used to provide an unbiased evaluation of a **final model** fit on the training dataset.\n",
    "\n",
    "By partitioning or splitting the available data into these three sets, we can avoid **overfitting** the model to the training data. The model is trained on the training set and evaluated on the validation set. The best model is chosen based on the performance on the validation set. The final performance of this best model is then evaluated on the test set, and this is the performance that we report.\n",
    "\n",
    "*<u>Note</u>: In the literature, the validation set is sometimes called the test set. This is a misnomer, as the test set is used to provide an **unbiased evaluation** of the final model fit on the training dataset. The validation set is used to **evaluate the model and tune its hyperparameters**. Even the documentation of scikit-learn uses the term test set to refer to the validation set. This is a source of confusion and should be avoided.*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Engineering Techniques\n",
    "\n",
    "Feature Engineering also helps to improve the performance and stability of the model. It is a process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\n",
    "\n",
    "See the **separate notebook** (Feature Engineering.ipynb) on Feature Engineering for more details and examples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Model\n",
    "We will use logistic regression as our model. Logistic regression is a linear model for classification. It is a good starting point for binary classification problems. If you like to learn more about logistic regression, you can read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "Note: despite its name, logistic regression is a linear model for **classification** rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n",
    "\n",
    "So, don't **confuse** logistic regression with linear regression!\n",
    "\n",
    "In the general case, you'll use multiple models to find the best one. You can use the **validation set** to evaluate the performance of the different models. The model with the best performance on the validation set is the one that you will use to make predictions on the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data splitting techniques"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Holdout\n",
    "Holdout is the simplest form of model validation. It is a good starting point when you are working with limited data. It is also the default validation method in scikit-learn, keras, and other machine learning libraries.\n",
    "\n",
    "The procedure is as follows:\n",
    "* Shuffle the dataset randomly.\n",
    "* Split the dataset into two groups, a rule of thumb is to use a 70%/30% split.\n",
    "* Take the first group as a training set and the second group as a hold out set, also called the test set.\n",
    "* Fit a model on the training set and evaluate it on the hold out set\n",
    "\n",
    "![](../images/holdout.png)\n",
    "\n",
    "The above procedure can be done by using the `train_test_split` function from scikit-learn. The function takes the dataset and the target as input and returns the training and test sets. The test_size parameter is used to specify the size of the test set. The random_state parameter is used to specify the random seed for shuffling the dataset. If you like to learn more about this function, you can read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (700, 5), (700,)\n",
      "Test set: (300, 5), (300,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(f'Training set: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Test set: {X_test.shape}, {y_test.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.90\n",
      "Holdout Accuracy: 0.89\n",
      "Our model performs quite well on the holdout set.\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "print(f'Train Accuracy: {model.score(X_train, y_train):.2f}')\n",
    "print(f'Holdout Accuracy: {model.score(X_test, y_test):.2f}')\n",
    "print('Our model performs quite well on the holdout set.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Holdout with validation set\n",
    "Holdout can be improved by adding a validation set. The procedure is as follows:\n",
    "\n",
    "* Shuffle the dataset randomly.\n",
    "* Split the dataset into three groups, a rule of thumb is to use a 60%/20%/20% split.\n",
    "* Take the first group as a training set.\n",
    "* Take the second group as a validation set.\n",
    "* Take the third group as a test set.\n",
    "* Fit multiple models on the training set and evaluate them on the validation set.\n",
    "* Choose the **best model** based on the performance on the validation set.\n",
    "* Evaluate the final model on the test set.\n",
    "* Report the performance on the test set as the performance of the model.\n",
    "\n",
    "![](../images/holdout%20with%20validation.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (600, 5), (600,)\n",
      "Validation set: (200, 5), (200,)\n",
      "Test set: (200, 5), (200,)\n"
     ]
    }
   ],
   "source": [
    "# apply train_test_split twice to create a train (60%), validation (20%) and test set (20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25)\n",
    "\n",
    "print(f'Training set: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Validation set: {X_val.shape}, {y_val.shape}')\n",
    "print(f'Test set: {X_test.shape}, {y_test.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## More than one validation set\n",
    "\n",
    "To reduce variability, multiple rounds of validation are performed using different partitions, and the validation results are averaged over the rounds. This is called **cross-validation**. The rounds are called **folds**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "            train  validation  difference\nAverage  0.894667      0.8615    0.033167\nFold 1   0.905000      0.8550    0.050000\nFold 2   0.900000      0.8400    0.060000\nFold 3   0.886667      0.8750    0.011667\nFold 4   0.913333      0.8400    0.073333\nFold 5   0.910000      0.8700    0.040000\nFold 6   0.898333      0.8850    0.013333\nFold 7   0.886667      0.9000   -0.013333\nFold 8   0.888333      0.8550    0.033333\nFold 9   0.883333      0.8700    0.013333\nFold 10  0.875000      0.8250    0.050000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>train</th>\n      <th>validation</th>\n      <th>difference</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Average</th>\n      <td>0.894667</td>\n      <td>0.8615</td>\n      <td>0.033167</td>\n    </tr>\n    <tr>\n      <th>Fold 1</th>\n      <td>0.905000</td>\n      <td>0.8550</td>\n      <td>0.050000</td>\n    </tr>\n    <tr>\n      <th>Fold 2</th>\n      <td>0.900000</td>\n      <td>0.8400</td>\n      <td>0.060000</td>\n    </tr>\n    <tr>\n      <th>Fold 3</th>\n      <td>0.886667</td>\n      <td>0.8750</td>\n      <td>0.011667</td>\n    </tr>\n    <tr>\n      <th>Fold 4</th>\n      <td>0.913333</td>\n      <td>0.8400</td>\n      <td>0.073333</td>\n    </tr>\n    <tr>\n      <th>Fold 5</th>\n      <td>0.910000</td>\n      <td>0.8700</td>\n      <td>0.040000</td>\n    </tr>\n    <tr>\n      <th>Fold 6</th>\n      <td>0.898333</td>\n      <td>0.8850</td>\n      <td>0.013333</td>\n    </tr>\n    <tr>\n      <th>Fold 7</th>\n      <td>0.886667</td>\n      <td>0.9000</td>\n      <td>-0.013333</td>\n    </tr>\n    <tr>\n      <th>Fold 8</th>\n      <td>0.888333</td>\n      <td>0.8550</td>\n      <td>0.033333</td>\n    </tr>\n    <tr>\n      <th>Fold 9</th>\n      <td>0.883333</td>\n      <td>0.8700</td>\n      <td>0.013333</td>\n    </tr>\n    <tr>\n      <th>Fold 10</th>\n      <td>0.875000</td>\n      <td>0.8250</td>\n      <td>0.050000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for _ in range(10):\n",
    "    # apply train_test_split twice to create a train (60%), validation (20%) and test set (20%)\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    val_score = model.score(X_val, y_val)\n",
    "    results.append([train_score, val_score, train_score - val_score])\n",
    "\n",
    "rounds = pd.DataFrame(results, columns=['train', 'validation', 'difference'], index=[f'Fold {i}' for i in range(1, 11)])\n",
    "pd.concat([rounds.mean().rename('Average').to_frame().T, rounds], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross-Validation\n",
    "Instead of doing the cross-validation manually as we did in the previous section we will use the `cross_val_score` function from scikit-learn to perform K-fold cross-validation. The function takes the model, the dataset, and the target as input and returns the scores for each fold. The cv parameter is used to specify the number of folds. The scoring parameter is used to specify the scoring metric. If you like to learn more about this function, you can read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html).\n",
    "\n",
    "The reason we call this the basic version is it makes several assumptions about the dataset that are not always true. The assumptions are:\n",
    "* The dataset is large enough.\n",
    "* The observations are independent of each other.\n",
    "* The observations are identically distributed.\n",
    "* The observations are randomly sampled from the dataset.\n",
    "* The observations are balanced.\n",
    "\n",
    "More on this later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6-Fold Cross-Validation Accuracy: 0.88 (+/- 0.02)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([0.89820359, 0.89820359, 0.88622754, 0.88622754, 0.84337349,\n       0.87951807])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=6)\n",
    "print(f'6-Fold Cross-Validation Accuracy: {scores.mean():.2f} (+/- {scores.std():.2f})')\n",
    "scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "   fit_time  score_time  test_score  train_score\n0  0.015114    0.000000    0.898204     0.887155\n1  0.005326    0.005132    0.898204     0.882353\n2  0.004780    0.004622    0.886228     0.889556\n3  0.005245    0.000000    0.886228     0.884754\n4  0.007154    0.000000    0.843373     0.887290\n5  0.000000    0.000000    0.879518     0.895683",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fit_time</th>\n      <th>score_time</th>\n      <th>test_score</th>\n      <th>train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.015114</td>\n      <td>0.000000</td>\n      <td>0.898204</td>\n      <td>0.887155</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.005326</td>\n      <td>0.005132</td>\n      <td>0.898204</td>\n      <td>0.882353</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.004780</td>\n      <td>0.004622</td>\n      <td>0.886228</td>\n      <td>0.889556</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.005245</td>\n      <td>0.000000</td>\n      <td>0.886228</td>\n      <td>0.884754</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.007154</td>\n      <td>0.000000</td>\n      <td>0.843373</td>\n      <td>0.887290</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.879518</td>\n      <td>0.895683</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores = cross_validate(model, X, y, cv=6, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-Fold Cross-Validation\n",
    "\n",
    "K-fold cross-validation is a technique where the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once.\n",
    "\n",
    "The procedure is as follows:\n",
    "* Shuffle the dataset randomly.\n",
    "* Split the dataset into $k$ groups\n",
    "* For each of the $k$ groups:\n",
    "    * Take a group as a holdout validation set\n",
    "    * Combine the remaining $k-1$ groups as a training data set\n",
    "    * Fit a model on the training set and evaluate it on the validation set\n",
    "    * Retain the evaluation score and discard the model\n",
    "    * Repeat for the next group\n",
    "* The performance score is the average of the values computed in the loop\n",
    "* The final model is evaluated on the test set\n",
    "\n",
    "![](../images/k_fold.png)\n",
    "\n",
    "If you like to learn more about this technique, you can read the [documentation](https://scikit-learn.org/stable/modules/cross_validation.html#k-fold).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6-Fold Cross-Validation Accuracy: 0.88 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=6, shuffle=True)\n",
    "scores = cross_val_score(model, X, y, cv=kf)\n",
    "print(f'6-Fold Cross-Validation Accuracy: {scores.mean():.2f} (+/- {scores.std():.2f})')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Group K-Fold Cross-Validation\n",
    "\n",
    "The `GroupKFold` function is a variation of the `KFold` function. The only difference is that the `GroupKFold` function makes sure that the same group is not in both the training and validation sets. This is useful when the observations are not independent of each other. For example, if we have a dataset of patients, and we want to predict whether a patient will have a heart attack, we can use the `GroupKFold` function to make sure that the **same patient** is not in both the training and validation sets. This is because we want to make sure that the model is not using information from the validation set to make predictions on the training set.\n",
    "\n",
    "We will create a new and very simple dataset to demonstrate this. This dataset has an extra column called `group`. The `group` column is just a random number between 0 and 4. We will use the `GroupKFold` function to make sure that the same group is not in both the training and validation sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "          X  group  y\n0   Subset1      1  1\n1   Subset2      1  0\n2   Subset3      2  1\n3   Subset4      2  0\n4   Subset5      3  0\n5   Subset6      3  1\n6   Subset7      4  1\n7   Subset8      4  0\n8   Subset9      5  1\n9  Subset10      5  0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>group</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Subset1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Subset2</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Subset3</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Subset4</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Subset5</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Subset6</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Subset7</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Subset8</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Subset9</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Subset10</td>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X = ['Subset1', 'Subset2', 'Subset3', 'Subset4', 'Subset5', 'Subset6', 'Subset7', 'Subset8', 'Subset9', 'Subset10']\n",
    "groups = [1, 1, 2, 2, 3, 3, 4, 4, 5, 5]\n",
    "y = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
    "data = pd.DataFrame({'X': X, 'group': groups, 'y': y})\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "       groups in train groups in val\nFold 0    [1, 2, 3, 4]           [5]\nFold 1    [1, 2, 3, 5]           [4]\nFold 2    [1, 2, 4, 5]           [3]\nFold 3    [1, 3, 4, 5]           [2]\nFold 4    [2, 3, 4, 5]           [1]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>groups in train</th>\n      <th>groups in val</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Fold 0</th>\n      <td>[1, 2, 3, 4]</td>\n      <td>[5]</td>\n    </tr>\n    <tr>\n      <th>Fold 1</th>\n      <td>[1, 2, 3, 5]</td>\n      <td>[4]</td>\n    </tr>\n    <tr>\n      <th>Fold 2</th>\n      <td>[1, 2, 4, 5]</td>\n      <td>[3]</td>\n    </tr>\n    <tr>\n      <th>Fold 3</th>\n      <td>[1, 3, 4, 5]</td>\n      <td>[2]</td>\n    </tr>\n    <tr>\n      <th>Fold 4</th>\n      <td>[2, 3, 4, 5]</td>\n      <td>[1]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpkfold = GroupKFold(n_splits=5)\n",
    "splits = pd.DataFrame()\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(grpkfold.split(X, y, groups=groups)):\n",
    "    splits = pd.concat([splits,\n",
    "                        pd.DataFrame({'groups in train': [data.iloc[train_index]['group'].unique()],\n",
    "                                      'groups in val': [data.iloc[val_index]['group'].unique()]}, index=[f'Fold {i}'])])\n",
    "splits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Time Series Cross-Validation\n",
    "Time series cross-validation is a special case of K-fold cross-validation where the observations are ordered by time. Think of stock market data. We want to make sure that we don't use future data to predict the past. This is why we need to use time series cross-validation. This also means that the observations should not be randomly sampled from the dataset.\n",
    "\n",
    "This method is not recommended to use _unless_ you have a time series dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x504 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "splits = pd.DataFrame()\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "    splits = pd.concat([splits,\n",
    "                        pd.DataFrame({'train': train_index.size,\n",
    "                                      'val': val_index.size}, index=[f'Fold {i}'])])\n",
    "\n",
    "_ = splits.plot(kind='barh', figsize=(10, 7), stacked=True, title='Time Series Cross-Validation')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Leave-One-Out Cross-Validation\n",
    "\n",
    "Leave-one-out cross-validation is a special case of K-fold cross-validation where K is equal to the number of observations in the dataset. This means that each observation is used as the validation set once. This method is very computationally expensive. It is not recommended to use this method unless you have a **very small** dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Leave-P-Out Cross-Validation\n",
    "\n",
    "Leave-P-out cross-validation is a special case of K-fold cross-validation where K is equal to the number of observations in the dataset minus P. This means that each observation is used as the validation set once. This method is very computationally expensive. It is not recommended to use this method unless you have a very small dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "\n",
    "> [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "> [Top 7 Cross-Validation Techniques with Python Code](https://www.analyticsvidhya.com/blog/2021/11/top-7-cross-validation-techniques-with-python-code/)\n",
    "> [Cross-Validation Techniques](https://medium.com/geekculture/cross-validation-techniques-33d389897878)\n",
    "> [Cross-Validation: A Gentle Introduction](https://machinelearningmastery.com/cross-validation/)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
